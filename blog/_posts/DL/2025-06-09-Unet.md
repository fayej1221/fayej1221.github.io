---
layout: post
title: DL| U-Net ë…¼ë¬¸ ë¦¬ë·° ë° êµ¬í˜„
description: > 
    U-Net ë…¼ë¬¸ ë¦¬ë·°ë¥¼ í†µí•´ êµ¬ì¡° ë° biomedical image segmentationì—ì„œì˜ ì‘ìš©ì„ ì •ë¦¬í•˜ê³ , PyTorch ê¸°ë°˜ êµ¬í˜„ìœ¼ë¡œ ì„¸ë¶€ êµ¬ì¡°ì™€ í•™ìŠµ ë°©ì‹ì„ ë¶„ì„
categories: [DL]
tags: [U-Net, Biomedical Image Segmentation]
---
[U-Net paper](https://arxiv.org/pdf/1505.04597)
BoostCampAITECH
{:.note title="ì¶œì²˜ ë° ì°¸ê³ "}

* this unordered seed list will be replaced by the toc
{:toc}

- deep learningì€ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ê³  ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì„œ train dataê°€ ë§ì´ í•„ìš”
- biomedicalì˜ íŠ¹ì„±ìƒ ë°ì´í„°ì˜ ìˆ˜ê°€ ë§ì´ ë¶€ì¡±(ì „ë¬¸ê°€ ë¼ë²¨ë§ ì‘ì—… ë° ê°œì¸ ì •ë³´ ë•Œë¬¸ì—)
- cell segmentataionì˜ ê²½ìš° ê°™ì€ í´ë˜ìŠ¤ê°€ ì¸ì ‘í•´ ìˆëŠ” ì…€ ì‚¬ì´ ê²½ê³„ êµ¬ë¶„ì´ í•„ìš”í•œë° ê°™ì€ í´ë˜ìŠ¤ ì§€ë§Œ ì„œë¡œ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ êµ¬ë¶„ì´ í•„ìš”í•¨

**â†’ ì¼ë°˜ì ì¸ semantic segmentation ì‘ì—…ì—ì„œëŠ” ë¶ˆê°€ëŠ¥**

# Abstract

deep net-works requires many thousand annotated training samples

- data augmentataion
- architecture
    - contracting path: to capture context
    - expanding path: that enables precies localization

# Introduction

## biomedical image

**classification task**: output to an image is a single class label

![image.png](../../../assets/img/Unet/image.png)

ë§ì€ visual tasks ì¤‘

**biomedical image**: ì´ë¯¸ì§€ ì•ˆì— ì—¬ëŸ¬ê°œì˜ ì„¸í¬ê°€ ë“¤ì–´ìˆê¸° ë•Œë¬¸ì— í”½ì…€ë³„ë¡œ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•´ì•¼í•˜ëŠ” ê²ƒì´ í•„ìš”(output should include localization(i.e. a class label is supposed to be assigned to each pixel.)

## ì´ì „ ë…¼ë¬¸ë“¤

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì—¬ëŸ¬ ë°©ì‹ì´ ë‚˜ì˜¤ëŠ”ë° unet ë…¼ë¬¸ì—ì„œëŠ” í•œ ê°€ì§€ë¥¼ ì–¸ê¸‰(EM segmentataion challenge at ISBI 2012ì—ì„œ ì´ê²¼ë˜ â€˜*Deep neural networks segment neuronal membranes in electron microscopy imagesâ€™*)

**sliding-windowì˜ ë°©ì‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê° í”½ì…€ ì£¼ìœ„ì˜ ì§€ì—­ ì˜ì—­(íŒ¨ì¹˜)ì„ ì…ë ¥ìœ¼ë¡œ ì œê³µí•¨ìœ¼ë¡œì¨ ê° í”½ì…€ì˜ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ê²Œ í•¨**

1. this network can localize
2. the training data in terms of patches is much larger than the number of training images

í•´ë‹¹ ë…¼ë¬¸ì˜ ë¬¸ì œì 

- **quite slow**
    - the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches.
- **trade-off between localization accuracy and the use of context**
    - larger patches require more max-pooling layers that reduce the localization accuracy
    - while small patches allow the network to see only little context

ìµœê·¼ ì ‘ê·¼ë²•ë“¤ì—ì„œëŠ” ì—¬ëŸ¬ ê³„ì¸µì˜ íŠ¹ì§•ì„ ë°˜ì˜í•˜ëŠ” ë¶„ë¥˜ê¸° ì¶œë ¥ì„ ì œì•ˆ, ì´ë¥¼ í†µí•´ì„œ good localization and the use of context are possible at the same timeì„ í•  ìˆ˜ ìˆìŒ

## unetì€ more elegant architecture

![image.png](../../../assets/img/Unet/image%201.png)

**â€˜Fully Convolutional Networks for Semantic Segmentationâ€™(FCN)[9]ë¥¼ ìˆ˜ì •í•˜ê³  í™•ì¥í•˜ì—¬ ì ì€ ì´ë¯¸ì§€, ì •í™•í•œ segmentationì´ ê°€ëŠ¥í•˜ë„ë¡ í•¨**

### [9]ì˜ main idea

1. usual contracting networkë¥¼ successive layersë¡œ ë³´ì™„í•˜ë©° pooling layerëŠ” upsampling layerë¡œ ëŒ€ì²´ â†’ **increase the resolution of the output**
2. order to localizeë¥¼ í•˜ê¸° ìœ„í•´ì„œ contracting pathì˜ high resolution faturesë¥¼ upsampled outputê³¼ combined **â†’ A successive convolution layer can then learn to assemble a more precise output based on this infomration**

## one important modification in our architecture

- **upsampling partì— large number fo feature channelsë¥¼ ë‘ **
    - which allow the network to propagate context information to higher resolution layers.
    - ê·¸ ê²°ê³¼, expansive pathëŠ” contracting pathì™€ ëŒ€ì¹­ì„ ì´ë£¨ë©´ì„œ Uìí˜• êµ¬ì¡°ë¡œ í˜•ì„±
- **overlap-tile strategy**
    - fully connected layersëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ
    - ê° convolutionì˜ ìœ íš¨í•œ ë¶€ë¶„ë§Œì„ ì‚¬ìš©í•¨ ì¦‰, segmentation mapì—ëŠ” ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ì „ì²´ contextê°€ í™•ë³´ëœ í”½ì…€ë§Œ í¬í•¨
    - ì„ì˜ í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ë§¤ë„ëŸ½ê²Œ ë¶„í• í•  ìˆ˜ ìˆê²Œ í•˜ë©° ì´ë¯¸ì§€ ê²½ê³„ í”½ì…€ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ë¶€ì¡±í•œ contextëŠ” Extrapolation ê¸°ë²•ì„ ì ìš© â†’ **ë„¤íŠ¸ì›Œí¬ë¥¼ ëŒ€í˜• ì´ë¯¸ì§€ì— ì ìš©í•˜ëŠ”ë° ì¤‘ìš”í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ í•´ìƒë„ê°€ GPU ë©”ëª¨ë¦¬ì— ì˜í•´ ì œí•œë¨**
        
        ![image.png](../../../assets/img/Unet/image%202.png)
        

### data augmentataion

ë°ì´í„°ì˜ ì–‘ì´ ì ê¸° ë•Œë¬¸ì— ë°ì´í„° ì¦ê°•ì„ í†µí•´ Noiseì— ê°•ê±´í•˜ë„ë¡ í•™ìŠµí•˜ì˜€ê³  Rotataion, Shift, Elastic destortion ë“±ì„ ì‚¬ìš©

## Network Architecture

![image.png](../../../assets/img/Unet/image%201.png)

### contracting path

Downsampling ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ feature mapì„ ë§Œë“¦, downsamplingì„ í•  ë•Œ ë§ˆë‹¤ ì±„ë„ì˜ ìˆ˜ë¥¼ 2ë°° ì¦ê°€ì‹œí‚¤ë©´ì„œ ì§„í–‰

1. 3x3 convolution layer + ReLu 
2. 3x3 convolution layer + ReLu
3. 2x2 Max-polling Layer(Stride 2)

- 1, 2 ì´í›„ì— + BatchNorm(No Padding, Stride 1)ë¥¼ êµ¬í˜„í•  ë•Œ ëŒ€ì²´ë¡œ ì ìš©ì‹œí‚´

### expansive path

Upsamplingê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ feature mapì„ ìƒì„±

1. 2x2 Deconvolution layer(Stride 2): Pytorchì˜ ConvTranspose2d
2. **Contracting pathì—ì„œ ë™ì¼ levelì˜ íŠ¹ì§•ë§µì„ ì¶”ì¶œí•˜ê³  í¬ê¸°ë¥¼ ë§ì¶”ê¸° ìœ„í•´ cropping, ì´ì „ layerì—ì„œ ìƒì„±ëœ feature mapê³¼ concat:** ë™ì¼í•œ Levelì—ì„œ contracting pathì™€ í¬ê¸°ê°€ ë‹¤ë¥¸ ì´ìœ ëŠ” ì—¬ëŸ¬ ë²ˆì˜ íŒ¨ë”©ì´ ì—†ëŠ” 3x3 convolution layerë¥¼ ì§€ë‚˜ë©´ì„œ feature mapì˜ í¬ê¸°ê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸
3. 3Ã—3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)
4. 3Ã—3 Convolution Layer + ReLu + BatchNorm (No Padding, Stride 1)

final layerì—ì„œëŠ” 1x1 convolutionì„ ì‚¬ìš©í•˜ì—¬ ê° 64ì°¨ì› íŠ¹ì§• ë²¡í„°ë¥¼ ì›í•˜ëŠ” í´ë˜ìŠ¤ ìˆ˜ë¡œ ë§¤í•‘, ë„¤íŠ¸ì›Œí¬ëŠ” ì´ 23ê°œì˜ convolution layerë¡œ êµ¬ì„±ë¨

# Training

## ë©”ëª¨ë¦¬ ìµœì í™”

- unpadded convolutionìœ¼ë¡œ ì¸í•´ì„œ output imageëŠ” ì¼ì •í•œ ê²½ê³„ í­ë§Œí¼ inputë³´ë‹¤ ë” ì‘ìŒ â†’ overhead ìµœì†Œí™”, GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©
    - í° ë°°ì¹˜ í¬ê¸°ë³´ë‹¤ í° input tilesë¥¼ ì„ í˜¸í•˜ì—¬ batch_sizeë¥¼ 1ë¡œ í•¨
    - ë†’ì€ momentum(0.99)ì„ ì‚¬ìš©
    
    â†’ ì´ì „ í•™ìŠµ ìƒ˜í”Œì´ í˜„ì¬ ìµœì í™” ë‹¨ê³„ì˜ ì—…ë°ì´íŠ¸ì— ì˜í–¥ì„ ë¯¸ì¹˜ë„ë¡ í•¨
    

## Loss function

**energy function**: final feature mapì— í”½ì…€ ë‹¨ìœ„ë¡œ soft-maxë¥¼ í•˜ê³  cross entropy loss functionì„ ì ìš©í•˜ëŠ” ì‹

x(feature mapì— ìˆëŠ” ê° í”½ì…€)ë¡œ ê° í”½ì…€ì—ì„œ ê³„ì‚°í•œ ê²ƒì„ ë‹¤ ë”í•œ ê²ƒ, w(x)ëŠ” weight mapì´ë¼ê³  í”½ì…€ ë³„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ê³¼í•˜ëŠ” ì—­í• 

![image.png](../../../assets/img/Unet/image%203.png)

**weight map**: to balance the class frequencies: ê° ì„¸í¬ ì‚¬ì´ì˜ ê²½ê³„ë¥¼ ì˜ í¬ì°©í•´ì•¼í•˜ë©° í•™ìŠµ ë°ì´í„°ì—ì„œ ê° í”½ì…€ë§ˆë‹¤ í´ë˜ìŠ¤ ë¶„í¬ê°€ ë‹¤ë¥¸ ì ì„ ê³ ë ¤í•˜ì—¬ weight mapì„ êµ¬í•´ í•™ìŠµì— ë°˜ì˜

- $$w_c(x)$$: classë³„ ë¹ˆë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
- $$d_1$$: ê°€ì¥ ì¸ì ‘í•œ cellê³¼ì˜ ê±°ë¦¬
- $$d_2$$: ë‘ë²ˆì§¸ë¡œ ì¸ì ‘í•œ cellê³¼ì˜ ê±°ë¦¬
- $$w_0 = 10$$, $$ğœ = 5$$

![image.png](../../../assets/img/Unet/image%204.png)

## good initialization of the weights in extremely important

ë…¸ë“œì˜ ê°œìˆ˜ê°€ Nì´ë¼ë©´ root(2/N)ì˜ í‘œì¤€í¸ì°¨ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì´ìš©í•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”

E.g. for a 3x3 convolution and 64 feature channles in the previous layer N= 9 * 64 = 576

## Data Augmentataion

ë°ì´í„° ìˆ˜ê°€ ì ì„ ë•Œ í•„ìˆ˜ì ì„, microsopical images(í˜„ë¯¸ê²½ ì´¬ì˜ ì´ë¯¸ì§€)ëŠ” shift and rotation invariance as well as robustness to deformations and gray value variations.

shift, rotation ì™¸ì— random-elastic deformationì´ë¼ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¦ê°•ì„ êµ¬í•œí•˜ì˜€ê³  random-elastic deformationì€ ì‘ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  segmentation networkë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ key conceptë¡œ ë³´ì¸ë‹¤ê³  í•¨

**êµ¬í˜„ìˆœì„œ**

![image.png](../../../assets/img/Unet/image%205.png)

# Experiments

EM Segmentation challenge

![image.png](../../../assets/img/Unet/image%206.png)

ISBI cell tracking challenge

![image.png](../../../assets/img/Unet/image%207.png)

# Conclusion

biomdical segmentataion applicationsì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„, í•©ë¦¬ì ì¸ í•™ìŠµì‹œê°„ì„ ê°€ì¡Œìœ¼ë©° U-Netì˜ êµ¬ì¡°ê°€ ë‹¤ì–‘í•œ taskì— ì‚¬ìš©ì´ ë  ê²ƒ

# U-Netì˜ í•œê³„ì 

- U-Netì€ ê¸°ë³¸ì ìœ¼ë¡œ ê¹Šì´ê°€ 4ë¡œ ê³ ì •ì´ ë˜ì–´ ìˆì–´ ë°ì´í„°ì…‹ë§ˆë‹¤ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ëª»í•˜ë©° ìµœì  ê¹Šì´ íƒìƒ‰ ë¹„ìš©ì´ ë†’ìŒ
- ë‹¨ìˆœí•œ Skip Connectionìœ¼ë¡œ ë™ì¼í•œ ê¹Šì´ë¥¼ ê°€ì§€ëŠ” Encoder Decoderë§Œ ì—°ê²°ë˜ëŠ” ì œí•œì ì¸ êµ¬ì¡°

# U-Net êµ¬í˜„

```python
class UNet(nn.Module):
    def __init__(self, num_classes=len(CLASSES)):
        super(UNet, self).__init__()
        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):
            layers = []
            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                                 kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]
            layers += [nn.BatchNorm2d(num_features=out_channels)]
            layers += [nn.ReLU()]

            cbr = nn.Sequential(*layers)
            return cbr

        # Contracting path
        # Encoder 1: êµ¬í˜„ì²´ì— ë”°ë¼ì„œ paddingì„ 1ë¡œ ì£¼ì–´ í¬ê¸° ê³ ì •í•˜ê¸°ë„ í•¨
        self.enc1_1 = CBR2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)
        self.enc1_2 = CBR2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)
        self.pool1 = nn.MaxPool2d(kernel_size=2)
				
				# Encoder 2
        self.enc2_1 = CBR2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=True)
        self.enc2_2 = CBR2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=True)
        self.pool2 = nn.MaxPool2d(kernel_size=2)
        
				# Encoder 3
        self.enc3_1 = CBR2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=True)
        self.enc3_2 = CBR2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=True)
        self.pool3 = nn.MaxPool2d(kernel_size=2)
        
				# Encoder 4
        self.enc4_1 = CBR2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=True)
        self.enc4_2 = CBR2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1, bias=True)
        self.pool4 = nn.MaxPool2d(kernel_size=2)
				
				# Encoder 5 || Decoder 5
        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=True)
        self.enc5_2 = CBR2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=True)
        self.unpool4 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2, padding=0, bias=True)

        self.dec4_2 = CBR2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1, bias=True)
        self.dec4_1 = CBR2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1, bias=True)

        self.unpool3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2, padding=0, bias=True)

        self.dec3_2 = CBR2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1, bias=True)
        self.dec3_1 = CBR2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=True)

        self.unpool2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2, padding=0, bias=True)

        self.dec2_2 = CBR2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1, bias=True)
        self.dec2_1 = CBR2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)

        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, bias=True)

        self.dec1_2 = CBR2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)
        self.dec1_1 = CBR2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)
        self.score_fr = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1, stride=1, padding=0, bias=True) # Output Segmentation map

    def forward(self, x):
        enc1_1 = self.enc1_1(x)
        enc1_2 = self.enc1_2(enc1_1)
        pool1 = self.pool1(enc1_2)

        enc2_1 = self.enc2_1(pool1)
        enc2_2 = self.enc2_2(enc2_1)
        pool2 = self.pool2(enc2_2)

        enc3_1 = self.enc3_1(pool2)
        enc3_2 = self.enc3_2(enc3_1)
        pool3 = self.pool3(enc3_2)

        enc4_1 = self.enc4_1(pool3)
        enc4_2 = self.enc4_2(enc4_1)
        pool4 = self.pool4(enc4_2)

        enc5_1 = self.enc5_1(pool4)
        enc5_2 = self.enc5_2(enc5_1)
			
        unpool4 = self.unpool4(enc5_2)
        **cat4 = torch.cat((unpool4, enc4_2), dim=1)**
        dec4_2 = self.dec4_2(cat4)
        dec4_1 = self.dec4_1(dec4_2)

        unpool3 = self.unpool3(dec4_1)
        **cat3 = torch.cat((unpool3, enc3_2), dim=1)**
        dec3_2 = self.dec3_2(cat3)
        dec3_1 = self.dec3_1(dec3_2)

        unpool2 = self.unpool2(dec3_1)
        **cat2 = torch.cat((unpool2, enc2_2), dim=1)**
        dec2_2 = self.dec2_2(cat2)
        dec2_1 = self.dec2_1(dec2_2)

        unpool1 = self.unpool1(dec2_1)
        **cat1 = torch.cat((unpool1, enc1_2), dim=1)**
        dec1_2 = self.dec1_2(cat1)
        dec1_1 = self.dec1_1(dec1_2)

        output = self.score_fr(dec1_1)
        return output
```

padding=â€™sameâ€™ì„ ì´ìš©í•˜ì—¬ convolution ì´í›„ì—ë„ feature mapì˜ í¬ê¸°ê°€ ìœ ì§€ë˜ë„ë¡ í•˜ì—¬ cropping ê³¼ì •ì„ ìƒëµ, ì›ë˜ëŠ” ë°‘ì˜ ì½”ë“œë¥¼ ì‚¬ìš©í•¨

![image.png](../../../assets/img/Unet/image%208.png)

![image.png](../../../assets/img/Unet/image%209.png)